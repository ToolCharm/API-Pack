{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_all = pd.read_csv('aarest_base_generated_instructions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365 url errors and 10696 json errors for a total of 12061.\n",
      "That plus other errors resulted in a final dataset size of 120358.\n",
      "The distribution of methods is:\n",
      "{'delete': 24346, 'get': 23601, 'patch': 23237, 'post': 37962, 'put': 23273}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import demjson3\n",
    "import validators\n",
    "from pprint import pprint\n",
    "\n",
    "regions = [\n",
    "    \"us-east-2\", \"us-east-1\", \"us-west-1\", \"us-west-2\", \"af-south-1\", \n",
    "    \"ap-east-1\", \"ap-south-2\", \"ap-southeast-3\", \"ap-southeast-4\", \n",
    "    \"ap-south-1\", \"ap-northeast-3\", \"ap-northeast-2\", \"ap-southeast-1\", \n",
    "    \"ap-southeast-2\", \"ap-northeast-1\", \"ca-central-1\", \"ca-west-1\", \n",
    "    \"eu-central-1\", \"eu-west-1\", \"eu-west-2\", \"eu-south-1\", \"eu-west-3\", \n",
    "    \"eu-south-2\", \"eu-north-1\", \"eu-central-2\", \"il-central-1\", \n",
    "    \"me-south-1\", \"me-central-1\", \"sa-east-1\", \"us-gov-east-1\", \n",
    "    \"us-gov-west-1\"\n",
    "]\n",
    "methods = ['get', 'post', 'put', 'delete', 'patch']\n",
    "\n",
    "tokens_to_add = set([\"<URLSTART>\", \"<URLEND>\", \"<PAYLOADSTART>\", \"<PAYLOADEND>\"])\n",
    "final_dataset = {\"instructions\": [], \"api_calls\": []}\n",
    "method_errors = 0\n",
    "url_errors = 0\n",
    "json_errors = 0\n",
    "method_counts = {\n",
    "    mth: 0 for mth in methods\n",
    "}\n",
    "\n",
    "def parse_method(curl_command, i):\n",
    "    method: str = re.findall(r'--request ([a-zA-Z]*)', curl_command)[0].lower()\n",
    "\n",
    "    if method not in methods:\n",
    "        return False, f\"{i} Invalid method: {method}\"\n",
    "    \n",
    "    return True, method\n",
    "\n",
    "def parse_url(curl_command, i):\n",
    "    url: str = re.findall(r'--url ([^\\s]*)', curl_command)[0]\n",
    "    url = url.strip(\"'\\\"\\\\\")\n",
    "\n",
    "    # one off fixes\n",
    "    url = re.sub(r'\\/%7Bregion%7D', random.choice(regions), url)\n",
    "\n",
    "    valid = validators.url(url)\n",
    "    if not valid:\n",
    "        return False, f\"{i} Invalid url: {url}\"\n",
    "    \n",
    "    # remove the base url, leaving only the endpoint\n",
    "    endpoint = re.sub(r'http[s]?:\\/\\/[^\\/\\s]*', '', url)\n",
    "\n",
    "    if len(endpoint) == 0:\n",
    "        return False, f\"{i} No endpoint found in url: {url}\"\n",
    "    \n",
    "    return True, endpoint\n",
    "\n",
    "def parse_payload(curl_command, method, i):\n",
    "    data: list[str] = re.findall(r\"--data [\\\\']*({.*})[\\\\']*\", curl_command)\n",
    "\n",
    "    if len(data) > 0:\n",
    "        data = data[0]\n",
    "        try:\n",
    "            data = re.sub(r'\\\\\\\\\"', r'\\\"', data)\n",
    "            data = demjson3.decode(data)\n",
    "            data = json.dumps(data)\n",
    "        except Exception as e:\n",
    "            return False, f\"{i} issue loading json: {str(e)} {data}\"\n",
    "        return True, data\n",
    "    else:\n",
    "        if method in ['post', 'put', 'patch']:\n",
    "            return False, f\"{i} No payload for method {method}\"\n",
    "        return True, \"\"\n",
    "    \n",
    "with open(\"parsing_errors.txt\", \"w\") as f:\n",
    "    for i in range(data_all.shape[0]):\n",
    "        row = data_all.iloc[i]\n",
    "\n",
    "        try:\n",
    "            if pd.isna(row['instructions']):\n",
    "                # then we are looking at part of those original 13200 that weren't batched and didn't split by quotes\n",
    "                instruction: str = row['instruction']\n",
    "                instruction = instruction.split(\"\\\"\")[1].split(\"\\\"\")[0]\n",
    "                curl_command: str = row['api_call']\n",
    "            else:\n",
    "                # then we are looking at the batched instructions which were already split by quotes\n",
    "                instruction: str = row['instructions']\n",
    "                curl_command: str = row['api_calls']\n",
    "\n",
    "            # verify that the bot didn't go meta with the instruction\n",
    "            if \"curl\" in instruction:\n",
    "                continue\n",
    "\n",
    "            # parse the curl command into the correct tokenized form\n",
    "            curl_command = curl_command.split(\"curl\")[1]\n",
    "            tokenized_command = []\n",
    "\n",
    "            method_success, method_result = parse_method(curl_command, i)\n",
    "            if method_success:\n",
    "                method_counts[method_result] += 1\n",
    "                tokenized_command.append(f\"<{method_result.upper()}>\")\n",
    "            else:\n",
    "                f.write(method_result)\n",
    "                method_errors += 1\n",
    "                continue\n",
    "\n",
    "            url_success, url_result = parse_url(curl_command, i)\n",
    "            if url_success:\n",
    "                tokenized_command.append(f\"<ENDPOINTSTART>{url_result}<ENDPOINTEND>\")\n",
    "            else:\n",
    "                f.write(url_result)\n",
    "                url_errors += 1\n",
    "                continue\n",
    "\n",
    "            payload_success, payload_result = parse_payload(curl_command, method_result, i)\n",
    "            if payload_success:\n",
    "                tokenized_command.append(f\"<PAYLOADSTART>{payload_result}<PAYLOADEND>\")\n",
    "            else:\n",
    "                f.write(payload_result)\n",
    "                json_errors += 1\n",
    "                continue\n",
    "\n",
    "            tokenized_command = \"\".join(tokenized_command)\n",
    "            final_dataset[\"instructions\"].append(instruction)\n",
    "            final_dataset[\"api_calls\"].append(tokenized_command)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            break\n",
    "\n",
    "print(f\"{url_errors} url errors and {json_errors} json errors for a total of {url_errors + json_errors}.\\nThat plus other errors resulted in a final dataset size of {len(final_dataset['instructions'])}.\\nThe distribution of methods is:\")\n",
    "pprint(method_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_dataset)\n",
    "final_df.to_csv(\"rest_base_model_dataset_tokenized_endpoints_allpayload.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ToolCharm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
